{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10aa4635-2e29-4ead-b30e-5c8e388d8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "923c41fb-6696-46ab-bb2d-fba3d2a0dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_yolo_annotations(file_path):\n",
    "#     annotations = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             annotations.append([float(x) for x in line.strip().split()])\n",
    "#     return annotations\n",
    "\n",
    "# def write_yolo_annotations(file_path, annotations, crop_size):\n",
    "#     with open(file_path, 'w') as file:\n",
    "#         for ann in annotations:\n",
    "#             koef = crop_size / FINAL_SIZE\n",
    "#             ann = [int(ann[0]), koef*ann[1], koef*ann[2], koef*ann[3], koef*ann[4]]\n",
    "#             file.write(' '.join(map(str, ann)) + '\\n')\n",
    "\n",
    "# def is_object_fully_contained(x, y, x_center_abs, y_center_abs, width_abs, height_abs, crop_size):\n",
    "#     return (x < x_center_abs - width_abs / 2 and\n",
    "#             x + crop_size > x_center_abs + width_abs / 2 and\n",
    "#             y < y_center_abs - height_abs / 2 and\n",
    "#             y + crop_size > y_center_abs + height_abs / 2)\n",
    "\n",
    "# def is_crop_without_objects(x, y, annotations, w, h, crop_size):\n",
    "#     for ann in annotations:\n",
    "#         _, x_center, y_center, width, height = ann\n",
    "#         x_center_abs = x_center * w\n",
    "#         y_center_abs = y_center * h\n",
    "#         width_abs = width * w\n",
    "#         height_abs = height * h\n",
    "\n",
    "#         if not (x + crop_size <= x_center_abs - width_abs / 2 or\n",
    "#                 x >= x_center_abs + width_abs / 2 or\n",
    "#                 y + crop_size <= y_center_abs - height_abs / 2 or\n",
    "#                 y >= y_center_abs + height_abs / 2):\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# def resize_annotations(annotations, original_size, target_size):\n",
    "#     scale_x = target_size[0] / original_size[0]\n",
    "#     scale_y = target_size[1] / original_size[1]\n",
    "#     resized_annotations = []\n",
    "#     for ann in annotations:\n",
    "#         cls, x_center, y_center, width, height = ann\n",
    "#         resized_annotations.append([\n",
    "#             cls,\n",
    "#             x_center * scale_x,\n",
    "#             y_center * scale_y,\n",
    "#             width * scale_x,\n",
    "#             height * scale_y\n",
    "#         ])\n",
    "#     return resized_annotations\n",
    "\n",
    "# def calculate_overlap_area(x1, y1, x2, y2, size):\n",
    "#     # Calculate the overlapping area between two squares\n",
    "#     dx = min(x1 + size, x2 + size) - max(x1, x2)\n",
    "#     dy = min(y1 + size, y2 + size) - max(y1, y2)\n",
    "#     if dx >= 0 and dy >= 0:\n",
    "#         return dx * dy\n",
    "#     return 0\n",
    "\n",
    "# def filter_crops(crops, threshold):\n",
    "#     filtered_crops = []\n",
    "#     for i, (img1, x1, y1) in enumerate(crops):\n",
    "#         add_crop = True\n",
    "#         for j, (img2, x2, y2) in enumerate(filtered_crops):\n",
    "#             overlap_area = calculate_overlap_area(x1, y1, x2, y2, img1.size[0])\n",
    "#             if overlap_area / (img1.size[0] * img1.size[1]) > threshold:\n",
    "#                 add_crop = False\n",
    "#                 break\n",
    "#         if add_crop:\n",
    "#             filtered_crops.append((img1, x1, y1))\n",
    "#     return filtered_crops\n",
    "\n",
    "# def process_crop(img, annotations, crop_size, x_offset, y_offset, w, h, final_size, base_name, output_dir, images_with_objects, images_without_objects):\n",
    "#     objects_in_crop = []\n",
    "#     for ann in annotations:\n",
    "#         cls, x_center, y_center, width, height = ann\n",
    "#         x_center_abs = x_center * w\n",
    "#         y_center_abs = y_center * h\n",
    "#         width_abs = width * w\n",
    "#         height_abs = height * h\n",
    "        \n",
    "#         if is_object_fully_contained(x_offset, y_offset, x_center_abs, y_center_abs, width_abs, height_abs, crop_size):\n",
    "#             new_x_center = (x_center_abs - x_offset) / crop_size\n",
    "#             new_y_center = (y_center_abs - y_offset) / crop_size\n",
    "#             new_width = width_abs / crop_size\n",
    "#             new_height = height_abs / crop_size\n",
    "#             objects_in_crop.append([cls, new_x_center, new_y_center, new_width, new_height])\n",
    "    \n",
    "#     resized_img = img.resize((final_size, final_size), Image.LANCZOS)\n",
    "#     if objects_in_crop:\n",
    "#         resized_annotations = resize_annotations(objects_in_crop, (crop_size, crop_size), (final_size, final_size))\n",
    "#         images_with_objects.append((resized_img, resized_annotations))\n",
    "#     elif is_crop_without_objects(x_offset, y_offset, annotations, w, h, crop_size):\n",
    "#         images_without_objects.append(resized_img)\n",
    "\n",
    "# def crop_image(image, annotations, output_dir, base_name):\n",
    "#     w, h = image.size\n",
    "\n",
    "#     for crop_size in CROP_SIZES:\n",
    "#         if h < crop_size or w < crop_size:\n",
    "#             print(\"base_name\", crop_size, base_name)\n",
    "#             continue  # Пропустить, если изображение меньше текущего crop_size\n",
    "        \n",
    "#         cropped_images = []\n",
    "        \n",
    "#         # Нарезаем изображение на NUM_CROPS случайных фрагментов\n",
    "#         while len(cropped_images) < NUM_CROPS:\n",
    "#             x = random.randint(0, w - crop_size)\n",
    "#             y = random.randint(0, h - crop_size)\n",
    "#             cropped_img = image.crop((x, y, x + crop_size, y + crop_size))\n",
    "#             cropped_images.append((cropped_img, x, y))\n",
    "        \n",
    "#         # Фильтруем фрагменты по критерию перекрытия\n",
    "#         cropped_images = filter_crops(cropped_images, OVERLAP_THRESHOLD)\n",
    "        \n",
    "#         images_with_objects = []\n",
    "#         images_without_objects = []\n",
    "        \n",
    "#         for img, x_offset, y_offset in cropped_images:\n",
    "#             process_crop(img, annotations, crop_size, x_offset, y_offset, w, h, FINAL_SIZE, base_name, output_dir, images_with_objects, images_without_objects)\n",
    "        \n",
    "#         selected_with_objects = random.sample(images_with_objects, min(NUM_WITH_OBJECTS, len(images_with_objects)))\n",
    "#         max_without_objects = K * len(selected_with_objects)\n",
    "#         selected_without_objects = random.sample(images_without_objects, min(max_without_objects, len(images_without_objects)))\n",
    "        \n",
    "#         for i, (img, objs) in enumerate(selected_with_objects):\n",
    "#             img_name = f\"{base_name}_{crop_size}_obj_{i}.jpg\"\n",
    "#             img.save(os.path.join(output_dir, img_name))\n",
    "#             ann_name = f\"{base_name}_{crop_size}_obj_{i}.txt\"\n",
    "#             write_yolo_annotations(os.path.join(output_dir, ann_name), objs, crop_size)\n",
    "        \n",
    "#         for i, img in enumerate(selected_without_objects):\n",
    "#             img_name = f\"{base_name}_{crop_size}_no-obj_{i}.jpg\"\n",
    "#             img.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "# def process_directory(image_dir, annotation_dir, output_dir):\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "#     for image_file in image_files:\n",
    "#         base_name = os.path.splitext(image_file)[0]\n",
    "#         image_path = os.path.join(image_dir, image_file)\n",
    "#         annotation_path = os.path.join(annotation_dir, base_name + '.txt')\n",
    "        \n",
    "#         if os.path.exists(annotation_path):\n",
    "#             image = Image.open(image_path)\n",
    "#             annotations = read_yolo_annotations(annotation_path)\n",
    "#             crop_image(image, annotations, output_dir, base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bc39699-9b6b-4361-ae88-1a50939d5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# from PIL import Image\n",
    "\n",
    "# def read_yolo_annotations(file_path):\n",
    "#     annotations = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             annotations.append([float(x) for x in line.strip().split()])\n",
    "#     return annotations\n",
    "\n",
    "# def write_yolo_annotations(file_path, annotations, crop_size):\n",
    "#     with open(file_path, 'w') as file:\n",
    "#         for ann in annotations:\n",
    "#             koef = crop_size / FINAL_SIZE\n",
    "#             ann = [int(ann[0]), koef*ann[1], koef*ann[2], koef*ann[3], koef*ann[4]]\n",
    "#             file.write(' '.join(map(str, ann)) + '\\n')\n",
    "\n",
    "# def is_object_fully_contained(x, y, x_center_abs, y_center_abs, width_abs, height_abs, crop_size):\n",
    "#     return (x < x_center_abs - width_abs / 2 and\n",
    "#             x + crop_size > x_center_abs + width_abs / 2 and\n",
    "#             y < y_center_abs - height_abs / 2 and\n",
    "#             y + crop_size > y_center_abs + height_abs / 2)\n",
    "\n",
    "# def is_crop_without_objects(x, y, annotations, w, h, crop_size):\n",
    "#     for ann in annotations:\n",
    "#         _, x_center, y_center, width, height = ann\n",
    "#         x_center_abs = x_center * w\n",
    "#         y_center_abs = y_center * h\n",
    "#         width_abs = width * w\n",
    "#         height_abs = height * h\n",
    "\n",
    "#         if not (x + crop_size <= x_center_abs - width_abs / 2 or\n",
    "#                 x >= x_center_abs + width_abs / 2 or\n",
    "#                 y + crop_size <= y_center_abs - height_abs / 2 or\n",
    "#                 y >= y_center_abs + height_abs / 2):\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# def resize_annotations(annotations, original_size, target_size):\n",
    "#     scale_x = target_size[0] / original_size[0]\n",
    "#     scale_y = target_size[1] / original_size[1]\n",
    "#     resized_annotations = []\n",
    "#     for ann in annotations:\n",
    "#         cls, x_center, y_center, width, height = ann\n",
    "#         resized_annotations.append([\n",
    "#             cls,\n",
    "#             x_center * scale_x,\n",
    "#             y_center * scale_y,\n",
    "#             width * scale_x,\n",
    "#             height * scale_y\n",
    "#         ])\n",
    "#     return resized_annotations\n",
    "\n",
    "# def calculate_overlap_area(x1, y1, x2, y2, size):\n",
    "#     dx = min(x1 + size, x2 + size) - max(x1, x2)\n",
    "#     dy = min(y1 + size, y2 + size) - max(y1, y2)\n",
    "#     if dx >= 0 and dy >= 0:\n",
    "#         return dx * dy\n",
    "#     return 0\n",
    "\n",
    "# def filter_crops(crops, threshold):\n",
    "#     filtered_crops = []\n",
    "#     for i, (img1, x1, y1) in enumerate(crops):\n",
    "#         add_crop = True\n",
    "#         for j, (img2, x2, y2) in enumerate(filtered_crops):\n",
    "#             overlap_area = calculate_overlap_area(x1, y1, x2, y2, img1.size[0])\n",
    "#             if overlap_area / (img1.size[0] * img1.size[1]) > threshold:\n",
    "#                 add_crop = False\n",
    "#                 break\n",
    "#         if add_crop:\n",
    "#             filtered_crops.append((img1, x1, y1))\n",
    "#     return filtered_crops\n",
    "\n",
    "# def process_crop(img, annotations, crop_size, x_offset, y_offset, w, h, final_size, base_name, output_dir, images_with_objects, images_without_objects):\n",
    "#     objects_in_crop = []\n",
    "#     for ann in annotations:\n",
    "#         cls, x_center, y_center, width, height = ann\n",
    "#         x_center_abs = x_center * w\n",
    "#         y_center_abs = y_center * h\n",
    "#         width_abs = width * w\n",
    "#         height_abs = height * h\n",
    "        \n",
    "#         if is_object_fully_contained(x_offset, y_offset, x_center_abs, y_center_abs, width_abs, height_abs, crop_size):\n",
    "#             new_x_center = (x_center_abs - x_offset) / crop_size\n",
    "#             new_y_center = (y_center_abs - y_offset) / crop_size\n",
    "#             new_width = width_abs / crop_size\n",
    "#             new_height = height_abs / crop_size\n",
    "#             objects_in_crop.append([cls, new_x_center, new_y_center, new_width, new_height])\n",
    "    \n",
    "#     resized_img = img.resize((final_size, final_size), Image.LANCZOS)\n",
    "#     if objects_in_crop:\n",
    "#         resized_annotations = resize_annotations(objects_in_crop, (crop_size, crop_size), (final_size, final_size))\n",
    "#         images_with_objects.append((resized_img, resized_annotations))\n",
    "#     elif is_crop_without_objects(x_offset, y_offset, annotations, w, h, crop_size):\n",
    "#         images_without_objects.append(resized_img)\n",
    "\n",
    "# def crop_image(image, annotations, output_dir, base_name):\n",
    "#     w, h = image.size\n",
    "\n",
    "#     # Проверяем, подходит ли изображение под основные размеры\n",
    "#     fits_in_any_size = False\n",
    "#     for crop_size in CROP_SIZES[:-1]:\n",
    "#         if h >= crop_size and w >= crop_size:\n",
    "#             fits_in_any_size = True\n",
    "#             break\n",
    "    \n",
    "#     # Если изображение не подходит под размеры [1024, 640, 512], используем размер 256\n",
    "#     if not fits_in_any_size:\n",
    "#         crop_sizes_to_use = [CROP_SIZES[-1]]\n",
    "#     else:\n",
    "#         crop_sizes_to_use = CROP_SIZES[:-1]\n",
    "\n",
    "#     for crop_size in crop_sizes_to_use:\n",
    "#         if h < crop_size or w < crop_size:\n",
    "#             continue  # Пропустить, если изображение меньше текущего crop_size\n",
    "        \n",
    "#         cropped_images = []\n",
    "        \n",
    "#         # Нарезаем изображение на NUM_CROPS случайных фрагментов\n",
    "#         while len(cropped_images) < NUM_CROPS:\n",
    "#             x = random.randint(0, w - crop_size)\n",
    "#             y = random.randint(0, h - crop_size)\n",
    "#             cropped_img = image.crop((x, y, x + crop_size, y + crop_size))\n",
    "#             cropped_images.append((cropped_img, x, y))\n",
    "        \n",
    "#         # Фильтруем фрагменты по критерию перекрытия\n",
    "#         cropped_images = filter_crops(cropped_images, OVERLAP_THRESHOLD)\n",
    "        \n",
    "#         images_with_objects = []\n",
    "#         images_without_objects = []\n",
    "        \n",
    "#         for img, x_offset, y_offset in cropped_images:\n",
    "#             process_crop(img, annotations, crop_size, x_offset, y_offset, w, h, FINAL_SIZE, base_name, output_dir, images_with_objects, images_without_objects)\n",
    "        \n",
    "#         selected_with_objects = random.sample(images_with_objects, min(NUM_WITH_OBJECTS, len(images_with_objects)))\n",
    "#         max_without_objects = K * len(selected_with_objects)\n",
    "#         selected_without_objects = random.sample(images_without_objects, min(max_without_objects, len(images_without_objects)))\n",
    "        \n",
    "#         for i, (img, objs) in enumerate(selected_with_objects):\n",
    "#             img_name = f\"{base_name}_{crop_size}_obj_{i}.jpg\"\n",
    "#             img.save(os.path.join(output_dir, img_name))\n",
    "#             ann_name = f\"{base_name}_{crop_size}_obj_{i}.txt\"\n",
    "#             write_yolo_annotations(os.path.join(output_dir, ann_name), objs, crop_size)\n",
    "        \n",
    "#         for i, img in enumerate(selected_without_objects):\n",
    "#             img_name = f\"{base_name}_{crop_size}_no-obj_{i}.jpg\"\n",
    "#             img.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "# def process_directory(image_dir, annotation_dir, output_dir):\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "#     for image_file in image_files:\n",
    "#         base_name = os.path.splitext(image_file)[0]\n",
    "#         image_path = os.path.join(image_dir, image_file)\n",
    "#         annotation_path = os.path.join(annotation_dir, base_name + '.txt')\n",
    "        \n",
    "#         if os.path.exists(annotation_path):\n",
    "#             image = Image.open(image_path)\n",
    "#             annotations = read_yolo_annotations(annotation_path)\n",
    "#             crop_image(image, annotations, output_dir, base_name)\n",
    "\n",
    "# # Константы\n",
    "# CROP_SIZES = [1024, 640, 512, 256]\n",
    "# NUM_CROPS = 120\n",
    "# NUM_WITH_OBJECTS = 4\n",
    "# K = 1  # Максимальное отношение числа изображений без объектов к числу изображений с объектами\n",
    "# FINAL_SIZE = 640\n",
    "# OVERLAP_THRESHOLD = 0.5  # Максимальный процент перекрытия\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d33181ec-e43a-4583-8437-3cc85823beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def read_yolo_annotations(file_path):\n",
    "    annotations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            annotations.append([float(x) for x in line.strip().split()])\n",
    "    return annotations\n",
    "\n",
    "def write_yolo_annotations(file_path, annotations, crop_size):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for ann in annotations:\n",
    "            koef = crop_size / FINAL_SIZE\n",
    "            ann = [int(ann[0]), koef*ann[1], koef*ann[2], koef*ann[3], koef*ann[4]]\n",
    "            file.write(' '.join(map(str, ann)) + '\\n')\n",
    "\n",
    "def is_object_fully_contained(x, y, x_center_abs, y_center_abs, width_abs, height_abs, crop_size):\n",
    "    return (x < x_center_abs - width_abs / 2 and\n",
    "            x + crop_size > x_center_abs + width_abs / 2 and\n",
    "            y < y_center_abs - height_abs / 2 and\n",
    "            y + crop_size > y_center_abs + height_abs / 2)\n",
    "\n",
    "def is_crop_without_objects(x, y, annotations, w, h, crop_size):\n",
    "    for ann in annotations:\n",
    "        _, x_center, y_center, width, height = ann\n",
    "        x_center_abs = x_center * w\n",
    "        y_center_abs = y_center * h\n",
    "        width_abs = width * w\n",
    "        height_abs = height * h\n",
    "\n",
    "        if not (x + crop_size <= x_center_abs - width_abs / 2 or\n",
    "                x >= x_center_abs + width_abs / 2 or\n",
    "                y + crop_size <= y_center_abs - height_abs / 2 or\n",
    "                y >= y_center_abs + height_abs / 2):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def resize_annotations(annotations, original_size, target_size):\n",
    "    scale_x = target_size[0] / original_size[0]\n",
    "    scale_y = target_size[1] / original_size[1]\n",
    "    resized_annotations = []\n",
    "    for ann in annotations:\n",
    "        cls, x_center, y_center, width, height = ann\n",
    "        resized_annotations.append([\n",
    "            cls,\n",
    "            x_center * scale_x,\n",
    "            y_center * scale_y,\n",
    "            width * scale_x,\n",
    "            height * scale_y\n",
    "        ])\n",
    "    return resized_annotations\n",
    "\n",
    "def calculate_overlap_area(x1, y1, x2, y2, size):\n",
    "    dx = min(x1 + size, x2 + size) - max(x1, x2)\n",
    "    dy = min(y1 + size, y2 + size) - max(y1, y2)\n",
    "    if dx >= 0 and dy >= 0:\n",
    "        return dx * dy\n",
    "    return 0\n",
    "\n",
    "def filter_crops(crops, threshold):\n",
    "    filtered_crops = []\n",
    "    for i, (img1, x1, y1) in enumerate(crops):\n",
    "        add_crop = True\n",
    "        for j, (img2, x2, y2) in enumerate(filtered_crops):\n",
    "            overlap_area = calculate_overlap_area(x1, y1, x2, y2, img1.size[0])\n",
    "            if overlap_area / (img1.size[0] * img1.size[1]) > threshold:\n",
    "                add_crop = False\n",
    "                break\n",
    "        if add_crop:\n",
    "            filtered_crops.append((img1, x1, y1))\n",
    "    return filtered_crops\n",
    "\n",
    "def process_crop(img, annotations, crop_size, x_offset, y_offset, w, h, final_size, base_name, output_dir, images_with_objects, images_without_objects):\n",
    "    objects_in_crop = []\n",
    "    for ann in annotations:\n",
    "        cls, x_center, y_center, width, height = ann\n",
    "        x_center_abs = x_center * w\n",
    "        y_center_abs = y_center * h\n",
    "        width_abs = width * w\n",
    "        height_abs = height * h\n",
    "        \n",
    "        if is_object_fully_contained(x_offset, y_offset, x_center_abs, y_center_abs, width_abs, height_abs, crop_size):\n",
    "            new_x_center = (x_center_abs - x_offset) / crop_size\n",
    "            new_y_center = (y_center_abs - y_offset) / crop_size\n",
    "            new_width = width_abs / crop_size\n",
    "            new_height = height_abs / crop_size\n",
    "            objects_in_crop.append([cls, new_x_center, new_y_center, new_width, new_height])\n",
    "    \n",
    "    resized_img = img.resize((final_size, final_size), Image.LANCZOS)\n",
    "    if objects_in_crop:\n",
    "        resized_annotations = resize_annotations(objects_in_crop, (crop_size, crop_size), (final_size, final_size))\n",
    "        images_with_objects.append((resized_img, resized_annotations))\n",
    "    elif is_crop_without_objects(x_offset, y_offset, annotations, w, h, crop_size):\n",
    "        images_without_objects.append(resized_img)\n",
    "\n",
    "def crop_image(image, annotations, output_dir, base_name):\n",
    "    w, h = image.size\n",
    "\n",
    "    fits_in_any_size = False\n",
    "    for crop_size in CROP_SIZES[:-1]:\n",
    "        if h >= crop_size and w >= crop_size:\n",
    "            fits_in_any_size = True\n",
    "            break\n",
    "    \n",
    "    if not fits_in_any_size:\n",
    "        crop_sizes_to_use = [CROP_SIZES[-1]]\n",
    "    else:\n",
    "        crop_sizes_to_use = CROP_SIZES[:-1]\n",
    "\n",
    "    for crop_size in crop_sizes_to_use:\n",
    "        if h < crop_size or w < crop_size:\n",
    "            continue \n",
    "        \n",
    "        cropped_images = []\n",
    "\n",
    "        \n",
    "        while len(cropped_images) < NUM_CROPS:\n",
    "            x = random.randint(0, w - crop_size)\n",
    "            y = random.randint(0, h - crop_size)\n",
    "            cropped_img = image.crop((x, y, x + crop_size, y + crop_size))\n",
    "            cropped_images.append((cropped_img, x, y))\n",
    "\n",
    "        \n",
    "        cropped_images = filter_crops(cropped_images, OVERLAP_THRESHOLD)\n",
    "        \n",
    "        images_with_objects = []\n",
    "        images_without_objects = []\n",
    "        \n",
    "        for img, x_offset, y_offset in cropped_images:\n",
    "            process_crop(img, annotations, crop_size, x_offset, y_offset, w, h, FINAL_SIZE, base_name, output_dir, images_with_objects, images_without_objects)\n",
    "        \n",
    "        selected_with_objects = random.sample(images_with_objects, min(NUM_WITH_OBJECTS, len(images_with_objects)))\n",
    "        max_without_objects = K * len(selected_with_objects)\n",
    "        selected_without_objects = random.sample(images_without_objects, min(max_without_objects, len(images_without_objects)))\n",
    "        \n",
    "        for i, (img, objs) in enumerate(selected_with_objects):\n",
    "            img_name = f\"{base_name}_{crop_size}_obj_{i}.jpg\"\n",
    "            img.save(os.path.join(output_dir, img_name))\n",
    "            ann_name = f\"{base_name}_{crop_size}_obj_{i}.txt\"\n",
    "            write_yolo_annotations(os.path.join(output_dir, ann_name), objs, crop_size)\n",
    "        \n",
    "        for i, img in enumerate(selected_without_objects):\n",
    "            img_name = f\"{base_name}_{crop_size}_no-obj_{i}.jpg\"\n",
    "            img.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "    \n",
    "    if not fits_in_any_size:\n",
    "        resized_img = image.resize((FINAL_SIZE, FINAL_SIZE), Image.LANCZOS)\n",
    "        resized_annotations = resize_annotations(annotations, (w, h), (FINAL_SIZE, FINAL_SIZE))\n",
    "        img_name = f\"{base_name}_original.jpg\"\n",
    "        ann_name = f\"{base_name}_original.txt\"\n",
    "        resized_img.save(os.path.join(output_dir, img_name))\n",
    "        write_yolo_annotations(os.path.join(output_dir, ann_name), resized_annotations, FINAL_SIZE)\n",
    "\n",
    "def process_directory(image_dir, annotation_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        annotation_path = os.path.join(annotation_dir, base_name + '.txt')\n",
    "        \n",
    "        if not os.path.exists(annotation_path):\n",
    "            open(annotation_path, 'w').close()\n",
    "\n",
    "        if os.path.exists(annotation_path):\n",
    "            image = Image.open(image_path)\n",
    "            annotations = read_yolo_annotations(annotation_path)\n",
    "            crop_image(image, annotations, output_dir, base_name)\n",
    "\n",
    "\n",
    "CROP_SIZES = [1024, 640, 512, 256]\n",
    "NUM_CROPS = 120\n",
    "NUM_WITH_OBJECTS = 4\n",
    "K = 1 \n",
    "FINAL_SIZE = 640\n",
    "OVERLAP_THRESHOLD = 0.5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86d5e45a-eb48-4f3b-9780-8018b99232f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SIZES = [1024, 640, 512, 256]\n",
    "NUM_CROPS = 120\n",
    "NUM_WITH_OBJECTS = 4\n",
    "K = 1 \n",
    "FINAL_SIZE = 640\n",
    "OVERLAP_THRESHOLD = 0.5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbc0c039-aac4-4b5d-ab2e-87b533af191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# image_path = '/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/dataset_mmdet/test_new/'\n",
    "# annotations_path = '/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/dataset_mmdet/test_new/'\n",
    "# output_dir = '/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/dataset_mmdet/test_new_cropped'\n",
    "\n",
    "image_path = '/Users/elinachertova/Downloads/NO_BPLA/'\n",
    "annotations_path = '/Users/elinachertova/Downloads/NO_BPLA/'\n",
    "output_dir = '/Users/elinachertova/Downloads/NO_BPLA_crop'\n",
    "process_directory(image_path, annotations_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65859eb8-c81e-424a-8f5a-553c7db9b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cropped_with_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d50f1061-3636-4818-8618-610ef731af47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104762"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "len(os.listdir('/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/train_cropped_cl2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b35c382b-acd7-42b9-b5b5-bcee83b31a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено: 27_1285.txt 1\n",
      "Удалено: 45_137.txt 2\n",
      "Удалено: 91_1788.txt 3\n",
      "Завершено.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e016158-02cb-42fc-a0d3-a5ee0a9a60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "478e547b-e52e-4070-aab1-5383a7cd29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_annotations(file_path):\n",
    "    annotations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            try:\n",
    "                label = int(float(parts[0]))\n",
    "                x_center, y_center, width, height = map(float, parts[1:])\n",
    "                annotations.append((label, x_center, y_center, width, height))\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing line '{line.strip()}': {e}\")\n",
    "    return annotations\n",
    "\n",
    "def plot_image_with_annotations(image_path, annotations, ax):\n",
    "    image = Image.open(image_path)\n",
    "    ax.imshow(image)\n",
    "    for annotation in annotations:\n",
    "        label, x_center, y_center, width, height = annotation\n",
    "        img_width, img_height = image.size\n",
    "        rect = patches.Rectangle(\n",
    "            ((x_center - width / 2) * img_width, (y_center - height / 2) * img_height),\n",
    "            width * img_width, height * img_height,\n",
    "            linewidth=1, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text((x_center - width / 2) * img_width, (y_center - height / 2) * img_height, str(label), color='white', backgroundcolor='red')\n",
    "\n",
    "def display_images_with_annotations(folder_path, batch_size=2):\n",
    "    images = [f for f in os.listdir(folder_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "    total_images = len(images)\n",
    "    \n",
    "    for i in range(0, total_images, batch_size):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        axs = axs.flatten()\n",
    "        for j in range(batch_size):\n",
    "            idx = i + j\n",
    "            if idx < total_images:\n",
    "                image_file = images[idx]\n",
    "                image_path = os.path.join(folder_path, image_file)\n",
    "                annotation_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "                annotation_path = os.path.join(folder_path, annotation_file)\n",
    "                if os.path.exists(annotation_path):\n",
    "                    annotations = load_yolo_annotations(annotation_path)\n",
    "                else:\n",
    "                    annotations = []\n",
    "                plot_image_with_annotations(image_path, annotations, axs[j])\n",
    "            axs[j].axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0b0b04-0481-4dae-947f-7373907f0d2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# folder_path = 'path_to_your_folder'\n",
    "output_dir = '/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/dataset_mmdet/test_new_cropped'\n",
    "display_images_with_annotations(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99d660-4ea9-42f2-8298-64868e4cdb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af590c2c-7886-4b83-8110-667dc0390143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e535d232-bd7e-4947-a877-35590dcfbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "\n",
    "data_dir = '/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/dataset_mmdet/test_cropped_last'\n",
    "destination_dir = '/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/blue_sky_no_annotations_test_smal'\n",
    "\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "\n",
    "image_files = []\n",
    "for ext in image_extensions:\n",
    "    image_files.extend(glob(os.path.join(data_dir, ext)))\n",
    "\n",
    "def is_blue_sky(image_path):\n",
    "    \"\"\"Check if the image is predominantly blue sky.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    lower_blue = np.array([90, 50, 50])\n",
    "    upper_blue = np.array([130, 255, 255])\n",
    "\n",
    "    mask = cv2.inRange(hsv_image, lower_blue, upper_blue)\n",
    "    \n",
    "    blue_ratio = np.sum(mask > 0) / (image.shape[0] * image.shape[1])\n",
    "    \n",
    "    return blue_ratio > 0.8\n",
    "\n",
    "def get_base_name(image_path):\n",
    "    filename = os.path.basename(image_path)\n",
    "    base_name = '_'.join(filename.split('_')[:-2])\n",
    "    return base_name\n",
    "    \n",
    "image_groups = defaultdict(list)\n",
    "for image_file in image_files:\n",
    "    base_name = get_base_name(image_file)\n",
    "    print(\"base_name\", base_name)\n",
    "    image_groups[base_name].append(image_file)\n",
    "\n",
    "\n",
    "for base_name, images in image_groups.items():\n",
    "    no_annotation_images = []\n",
    "    \n",
    "    for image_file in images:\n",
    "        annotation_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "        \n",
    "        if not os.path.exists(annotation_file):\n",
    "            if is_blue_sky(image_file):\n",
    "                no_annotation_images.append(image_file)\n",
    "\n",
    "    \n",
    "    if len(no_annotation_images) > 1:\n",
    "        for image_to_move in no_annotation_images[1:]:\n",
    "            destination_path = os.path.join(destination_dir, os.path.basename(image_to_move))\n",
    "            print(f\"Moving image without annotation: {image_to_move} to {destination_path}\")\n",
    "            shutil.move(image_to_move, destination_path)\n",
    "\n",
    "print(\"Cleanup completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd79531-2ddb-47e5-acd9-ae209334502d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997ddfb-c322-46c2-91c2-abb92c734c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f65b4-df34-4302-a97c-5204ded79f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca78dfa-f18b-4c5c-92a3-b0412acb953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from glob import glob\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Path to the directory containing images and annotations\n",
    "# data_dir = '/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/train_cropped'\n",
    "\n",
    "# # Supported image file extensions\n",
    "# image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "\n",
    "# # Get a list of all image files with supported extensions\n",
    "# image_files = []\n",
    "# for ext in image_extensions:\n",
    "#     image_files.extend(glob(os.path.join(data_dir, ext)))\n",
    "\n",
    "# def is_blue_sky(image_path):\n",
    "#     \"\"\"Check if the image is predominantly blue sky.\"\"\"\n",
    "#     image = cv2.imread(image_path)\n",
    "#     hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "#     # Define range for blue color in HSV\n",
    "#     lower_blue = np.array([90, 50, 50])\n",
    "#     upper_blue = np.array([130, 255, 255])\n",
    "\n",
    "#     # Create a mask for blue color\n",
    "#     mask = cv2.inRange(hsv_image, lower_blue, upper_blue)\n",
    "    \n",
    "#     # Calculate the percentage of blue pixels\n",
    "#     blue_ratio = np.sum(mask > 0) / (image.shape[0] * image.shape[1])\n",
    "    \n",
    "#     # If more than 80% of the image is blue, consider it as blue sky\n",
    "#     return blue_ratio > 0.8\n",
    "\n",
    "# # Function to get the base name of the image (excluding the unique suffix)\n",
    "# def get_base_name(image_path):\n",
    "#     filename = os.path.basename(image_path)\n",
    "#     base_name = '_'.join(filename.split('_')[:-2])\n",
    "#     print(\"base_name\", base_name)\n",
    "#     return base_name\n",
    "\n",
    "# # Group images by their base names\n",
    "# image_groups = defaultdict(list)\n",
    "# for image_file in image_files:\n",
    "#     base_name = get_base_name(image_file)\n",
    "#     image_groups[base_name].append(image_file)\n",
    "\n",
    "# # Process each group\n",
    "# for base_name, images in image_groups.items():\n",
    "#     no_annotation_images = []\n",
    "    \n",
    "#     for image_file in images:\n",
    "#         annotation_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "        \n",
    "#         # Check if annotation file exists\n",
    "#         if not os.path.exists(annotation_file):\n",
    "#             # Check if the image is predominantly blue sky\n",
    "#             if is_blue_sky(image_file):\n",
    "#                 no_annotation_images.append(image_file)\n",
    "    \n",
    "#     # If there are multiple blue sky images without annotations, delete all but one\n",
    "#     if len(no_annotation_images) > 1:\n",
    "#         # Keep the first one and delete the rest\n",
    "#         for image_to_delete in no_annotation_images[1:]:\n",
    "#             print(f\"Deleting image without annotation: {image_to_delete}\")\n",
    "#             # os.remove(image_to_delete)\n",
    "\n",
    "# print(\"Cleanup completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b1bbe-b982-4fb7-9a66-060e665164af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67821344-2d3d-4298-be4a-da2dd2ee4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def crop_image(image, output_dir, base_name):\n",
    "    w, h = image.size\n",
    "\n",
    "    fits_in_any_size = False\n",
    "    for crop_size in CROP_SIZES[:-1]:\n",
    "        if h >= crop_size and w >= crop_size:\n",
    "            fits_in_any_size = True\n",
    "            break\n",
    "    \n",
    "    if not fits_in_any_size:\n",
    "        crop_sizes_to_use = [CROP_SIZES[-1]]\n",
    "    else:\n",
    "        crop_sizes_to_use = CROP_SIZES[:-1]\n",
    "\n",
    "    for crop_size in crop_sizes_to_use:\n",
    "        if h < crop_size or w < crop_size:\n",
    "            continue  \n",
    "            \n",
    "        cropped_images = []\n",
    "        \n",
    "        while len(cropped_images) < NUM_CROPS:\n",
    "            x = random.randint(0, w - crop_size)\n",
    "            y = random.randint(0, h - crop_size)\n",
    "            cropped_img = image.crop((x, y, x + crop_size, y + crop_size))\n",
    "            cropped_images.append((cropped_img, x, y))\n",
    "        \n",
    "        cropped_images = filter_crops(cropped_images, OVERLAP_THRESHOLD)\n",
    "        \n",
    "        for i, (img, x_offset, y_offset) in enumerate(cropped_images):\n",
    "            resized_img = img.resize((FINAL_SIZE, FINAL_SIZE), Image.LANCZOS)\n",
    "            img_name = f\"{base_name}_{crop_size}_crop_{i}.jpg\"\n",
    "            resized_img.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "    if not fits_in_any_size:\n",
    "        resized_img = image.resize((FINAL_SIZE, FINAL_SIZE), Image.LANCZOS)\n",
    "        img_name = f\"{base_name}_original.jpg\"\n",
    "        resized_img.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "def filter_crops(crops, threshold):\n",
    "    filtered_crops = []\n",
    "    for i, (img1, x1, y1) in enumerate(crops):\n",
    "        add_crop = True\n",
    "        for j, (img2, x2, y2) in enumerate(filtered_crops):\n",
    "            overlap_area = calculate_overlap_area(x1, y1, x2, y2, img1.size[0])\n",
    "            if overlap_area / (img1.size[0] * img1.size[1]) > threshold:\n",
    "                add_crop = False\n",
    "                break\n",
    "        if add_crop:\n",
    "            filtered_crops.append((img1, x1, y1))\n",
    "    return filtered_crops\n",
    "\n",
    "def calculate_overlap_area(x1, y1, x2, y2, size):\n",
    "    dx = min(x1 + size, x2 + size) - max(x1, x2)\n",
    "    dy = min(y1 + size, y2 + size) - max(y1, y2)\n",
    "    if dx >= 0 and dy >= 0:\n",
    "        return dx * dy\n",
    "    return 0\n",
    "\n",
    "def process_directory(image_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = Image.open(image_path)\n",
    "        crop_image(image, output_dir, base_name)\n",
    "\n",
    "\n",
    "CROP_SIZES = [1024, 640, 512, 256]\n",
    "NUM_CROPS = 20\n",
    "FINAL_SIZE = 640\n",
    "OVERLAP_THRESHOLD = 0.5  \n",
    "\n",
    "\n",
    "image_dir = '/Users/elinachertova/Downloads/NO_BPLA' \n",
    "output_dir = 'Users/elinachertova/Downloads/NO_BPLA_сrop' \n",
    "\n",
    "process_directory(image_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b32ee8-ac03-4589-991c-5c46736066ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3838202-7664-4019-9f42-cb2871d7bc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252368"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "len(os.listdir(\"/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/train_cropped\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a8c8be-ec5d-4edb-93c0-ed31ef6cb264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество изображений в папке /Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/train_cropped: 157993\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '/Users/elinachertova/Downloads/hackathon_additional_dataset/dataset/train_cropped'\n",
    "\n",
    "\n",
    "image_extensions = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "def count_images(folder_path, extensions):\n",
    "    count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(extensions):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "image_count = count_images(folder_path, image_extensions)\n",
    "\n",
    "print(f\"Количество изображений в папке {folder_path}: {image_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52750f69-8a97-4603-a332-36c1f94966de",
   "metadata": {},
   "outputs": [],
   "source": [
    "od."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
